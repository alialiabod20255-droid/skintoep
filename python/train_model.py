"""
Ù…Ø´Ø±ÙˆØ¹ ØªØµÙ†ÙŠÙ Ø§Ù„Ø£Ù…Ø±Ø§Ø¶ Ø§Ù„Ø¬Ù„Ø¯ÙŠØ© Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… PyTorch
Skin Disease Classification using PyTorch

Ù‡Ø°Ø§ Ø§Ù„Ù…Ù„Ù ÙŠØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ Ø§Ù„ÙƒÙˆØ¯ Ø§Ù„ÙƒØ§Ù…Ù„ Ù„ØªØ¯Ø±ÙŠØ¨ Ù†Ù…ÙˆØ°Ø¬ ØªØµÙ†ÙŠÙ Ø§Ù„Ø£Ù…Ø±Ø§Ø¶ Ø§Ù„Ø¬Ù„Ø¯ÙŠØ©
Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ù…Ø¬Ù…ÙˆØ¹Ø© Ø¨ÙŠØ§Ù†Ø§Øª HAM10000 ÙˆÙ†Ù…ÙˆØ°Ø¬ ResNet50 Ù…Ø¹ Transfer Learning
"""

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, Dataset
import torchvision.transforms as transforms
import torchvision.models as models
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from PIL import Image
import os
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, confusion_matrix
from tqdm import tqdm
import warnings
warnings.filterwarnings('ignore')

# Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„Ø¬Ù‡Ø§Ø² (GPU Ø£Ùˆ CPU)
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(f'Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø¬Ù‡Ø§Ø²: {device}')

class SkinDiseaseDataset(Dataset):
    """
    ÙØ¦Ø© Ù…Ø®ØµØµØ© Ù„ØªØ­Ù…ÙŠÙ„ Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø£Ù…Ø±Ø§Ø¶ Ø§Ù„Ø¬Ù„Ø¯ÙŠØ©
    Custom Dataset class for skin disease images
    """
    def __init__(self, dataframe, image_dir, transform=None):
        self.dataframe = dataframe
        self.image_dir = image_dir
        self.transform = transform
        
        # ØªØ¹Ø±ÙŠÙ Ø§Ù„ÙØ¦Ø§Øª Ø§Ù„Ø³Ø¨Ø¹ Ù„Ù„Ø£Ù…Ø±Ø§Ø¶ Ø§Ù„Ø¬Ù„Ø¯ÙŠØ©
        self.classes = {
            'nv': 0,    # Melanocytic nevi
            'mel': 1,   # Melanoma
            'bkl': 2,   # Benign keratosis-like lesions
            'bcc': 3,   # Basal cell carcinoma
            'akiec': 4, # Actinic keratoses
            'vasc': 5,  # Vascular lesions
            'df': 6     # Dermatofibroma
        }
        
        # Ø£Ø³Ù…Ø§Ø¡ Ø§Ù„ÙØ¦Ø§Øª Ø¨Ø§Ù„Ù„ØºØ© Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©
        self.class_names = {
            0: 'Ø§Ù„Ø´Ø§Ù…Ø§Øª Ø§Ù„ØµØ¨ØºÙŠØ©',
            1: 'Ø§Ù„ÙˆØ±Ù… Ø§Ù„Ù…ÙŠÙ„Ø§Ù†ÙŠÙ†ÙŠ Ø§Ù„Ø®Ø¨ÙŠØ«',
            2: 'Ø§Ù„Ø¢ÙØ§Øª Ø§Ù„Ø­Ù…ÙŠØ¯Ø© Ø§Ù„Ø´Ø¨ÙŠÙ‡Ø© Ø¨Ø§Ù„ØªÙ‚Ø±Ù†',
            3: 'Ø³Ø±Ø·Ø§Ù† Ø§Ù„Ø®Ù„Ø§ÙŠØ§ Ø§Ù„Ù‚Ø§Ø¹Ø¯ÙŠØ©',
            4: 'Ø§Ù„ØªÙ‚Ø±Ù† Ø§Ù„Ø´Ø¹Ø§Ø¹ÙŠ',
            5: 'Ø§Ù„Ø¢ÙØ§Øª Ø§Ù„ÙˆØ¹Ø§Ø¦ÙŠØ©',
            6: 'Ø§Ù„ÙˆØ±Ù… Ø§Ù„Ù„ÙŠÙÙŠ Ø§Ù„Ø¬Ù„Ø¯ÙŠ'
        }
    
    def __len__(self):
        return len(self.dataframe)
    
    def __getitem__(self, idx):
        # Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø§Ø³Ù… Ø§Ù„ØµÙˆØ±Ø© ÙˆØ§Ù„ØªØµÙ†ÙŠÙ
        image_name = self.dataframe.iloc[idx]['image_id'] + '.jpg'
        label = self.dataframe.iloc[idx]['dx']
        
        # ØªØ­Ù…ÙŠÙ„ Ø§Ù„ØµÙˆØ±Ø©
        image_path = os.path.join(self.image_dir, image_name)
        image = Image.open(image_path).convert('RGB')
        
        # ØªØ·Ø¨ÙŠÙ‚ Ø§Ù„ØªØ­ÙˆÙŠÙ„Ø§Øª
        if self.transform:
            image = self.transform(image)
        
        # ØªØ­ÙˆÙŠÙ„ Ø§Ù„ØªØµÙ†ÙŠÙ Ø¥Ù„Ù‰ Ø±Ù‚Ù…
        label = self.classes[label]
        
        return image, label

def load_and_explore_data():
    """
    ØªØ­Ù…ÙŠÙ„ ÙˆØ§Ø³ØªÙƒØ´Ø§Ù Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
    Load and explore the HAM10000 dataset
    """
    print("=" * 50)
    print("Ø§Ù„Ø®Ø·ÙˆØ© 1: ØªØ­Ù…ÙŠÙ„ ÙˆØ§Ø³ØªÙƒØ´Ø§Ù Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª")
    print("Step 1: Loading and Exploring Data")
    print("=" * 50)
    
    # ØªØ­Ù…ÙŠÙ„ Ù…Ù„Ù Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ÙˆØµÙÙŠØ©
    # ÙÙŠ Ø§Ù„ÙˆØ§Ù‚Ø¹ØŒ Ø³ØªØ­ØªØ§Ø¬ Ù„ØªØ­Ù…ÙŠÙ„ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø¨ÙŠØ§Ù†Ø§Øª HAM10000 Ù…Ù†:
    # https://www.kaggle.com/kmader/skin-cancer-mnist-ham10000
    
    # Ø¥Ù†Ø´Ø§Ø¡ Ø¨ÙŠØ§Ù†Ø§Øª ØªØ¬Ø±ÙŠØ¨ÙŠØ© Ù„Ù„ØªÙˆØ¶ÙŠØ­
    # ÙÙŠ Ø§Ù„ØªØ·Ø¨ÙŠÙ‚ Ø§Ù„Ø­Ù‚ÙŠÙ‚ÙŠØŒ Ø§Ø³ØªØ¨Ø¯Ù„ Ù‡Ø°Ø§ Ø¨ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ÙØ¹Ù„ÙŠØ©
    sample_data = {
        'image_id': [f'ISIC_{i:07d}' for i in range(1000)],
        'dx': np.random.choice(['nv', 'mel', 'bkl', 'bcc', 'akiec', 'vasc', 'df'], 1000),
        'age': np.random.randint(20, 80, 1000),
        'sex': np.random.choice(['male', 'female'], 1000)
    }
    
    df = pd.DataFrame(sample_data)
    
    print(f"Ø¹Ø¯Ø¯ Ø§Ù„ØµÙˆØ± Ø§Ù„Ø¥Ø¬Ù…Ø§Ù„ÙŠ: {len(df)}")
    print(f"Ø¹Ø¯Ø¯ Ø§Ù„ÙØ¦Ø§Øª: {df['dx'].nunique()}")
    print("\nØªÙˆØ²ÙŠØ¹ Ø§Ù„ÙØ¦Ø§Øª:")
    print(df['dx'].value_counts())
    
    # Ø±Ø³Ù… Ø¨ÙŠØ§Ù†ÙŠ Ù„ØªÙˆØ²ÙŠØ¹ Ø§Ù„ÙØ¦Ø§Øª
    plt.figure(figsize=(12, 6))
    df['dx'].value_counts().plot(kind='bar')
    plt.title('ØªÙˆØ²ÙŠØ¹ ÙØ¦Ø§Øª Ø§Ù„Ø£Ù…Ø±Ø§Ø¶ Ø§Ù„Ø¬Ù„Ø¯ÙŠØ©')
    plt.xlabel('Ù†ÙˆØ¹ Ø§Ù„Ù…Ø±Ø¶')
    plt.ylabel('Ø¹Ø¯Ø¯ Ø§Ù„ØµÙˆØ±')
    plt.xticks(rotation=45)
    plt.tight_layout()
    plt.savefig('class_distribution.png')
    plt.show()
    
    return df

def create_data_transforms():
    """
    Ø¥Ù†Ø´Ø§Ø¡ ØªØ­ÙˆÙŠÙ„Ø§Øª Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù„Ù„ØªØ¯Ø±ÙŠØ¨ ÙˆØ§Ù„Ø§Ø®ØªØ¨Ø§Ø±
    Create data transforms for training and testing
    """
    print("\n" + "=" * 50)
    print("Ø§Ù„Ø®Ø·ÙˆØ© 2: Ø¥Ø¹Ø¯Ø§Ø¯ ØªØ­ÙˆÙŠÙ„Ø§Øª Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª")
    print("Step 2: Setting up Data Transforms")
    print("=" * 50)
    
    # ØªØ­ÙˆÙŠÙ„Ø§Øª Ø§Ù„ØªØ¯Ø±ÙŠØ¨ (Ù…Ø¹ Data Augmentation)
    train_transforms = transforms.Compose([
        transforms.Resize((224, 224)),
        transforms.RandomHorizontalFlip(p=0.5),
        transforms.RandomVerticalFlip(p=0.5),
        transforms.RandomRotation(20),
        transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.485, 0.456, 0.406], 
                           std=[0.229, 0.224, 0.225])
    ])
    
    # ØªØ­ÙˆÙŠÙ„Ø§Øª Ø§Ù„Ø§Ø®ØªØ¨Ø§Ø± (Ø¨Ø¯ÙˆÙ† Augmentation)
    test_transforms = transforms.Compose([
        transforms.Resize((224, 224)),
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.485, 0.456, 0.406], 
                           std=[0.229, 0.224, 0.225])
    ])
    
    print("âœ… ØªÙ… Ø¥Ø¹Ø¯Ø§Ø¯ ØªØ­ÙˆÙŠÙ„Ø§Øª Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø¨Ù†Ø¬Ø§Ø­")
    print("- Ø­Ø¬Ù… Ø§Ù„ØµÙˆØ±Ø©: 224x224")
    print("- ØªØ·Ø¨ÙŠÙ‚ Data Augmentation Ø¹Ù„Ù‰ Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ØªØ¯Ø±ÙŠØ¨")
    print("- ØªØ·Ø¨ÙŠÙ‚ Normalization Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ù…Ø¹Ø§ÙŠÙŠØ± ImageNet")
    
    return train_transforms, test_transforms

def create_model():
    """
    Ø¥Ù†Ø´Ø§Ø¡ Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„ØªØµÙ†ÙŠÙ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Transfer Learning
    Create classification model using Transfer Learning
    """
    print("\n" + "=" * 50)
    print("Ø§Ù„Ø®Ø·ÙˆØ© 3: Ø¨Ù†Ø§Ø¡ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬")
    print("Step 3: Building the Model")
    print("=" * 50)
    
    # ØªØ­Ù…ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ ResNet50 Ø§Ù„Ù…Ø¯Ø±Ø¨ Ù…Ø³Ø¨Ù‚Ø§Ù‹
    model = models.resnet50(pretrained=True)
    
    # ØªØ¬Ù…ÙŠØ¯ Ø·Ø¨Ù‚Ø§Øª Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…Ø¯Ø±Ø¨ Ù…Ø³Ø¨Ù‚Ø§Ù‹
    for param in model.parameters():
        param.requires_grad = False
    
    # Ø§Ø³ØªØ¨Ø¯Ø§Ù„ Ø§Ù„Ø·Ø¨Ù‚Ø© Ø§Ù„Ø£Ø®ÙŠØ±Ø© Ù„Ù„ØªØµÙ†ÙŠÙ Ø¥Ù„Ù‰ 7 ÙØ¦Ø§Øª
    num_features = model.fc.in_features
    model.fc = nn.Sequential(
        nn.Dropout(0.5),
        nn.Linear(num_features, 512),
        nn.ReLU(),
        nn.Dropout(0.3),
        nn.Linear(512, 7)  # 7 ÙØ¦Ø§Øª Ù„Ù„Ø£Ù…Ø±Ø§Ø¶ Ø§Ù„Ø¬Ù„Ø¯ÙŠØ©
    )
    
    # Ù†Ù‚Ù„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¥Ù„Ù‰ Ø§Ù„Ø¬Ù‡Ø§Ø² Ø§Ù„Ù…Ù†Ø§Ø³Ø¨
    model = model.to(device)
    
    print("âœ… ØªÙ… Ø¨Ù†Ø§Ø¡ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¨Ù†Ø¬Ø§Ø­")
    print(f"- Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ø£Ø³Ø§Ø³ÙŠ: ResNet50")
    print(f"- Ø¹Ø¯Ø¯ Ø§Ù„ÙØ¦Ø§Øª: 7")
    print(f"- Ø§Ø³ØªØ®Ø¯Ø§Ù… Transfer Learning")
    
    return model

def train_model(model, train_loader, val_loader, num_epochs=25):
    """
    ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬
    Train the model
    """
    print("\n" + "=" * 50)
    print("Ø§Ù„Ø®Ø·ÙˆØ© 4: ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬")
    print("Step 4: Training the Model")
    print("=" * 50)
    
    # ØªØ¹Ø±ÙŠÙ Ø¯Ø§Ù„Ø© Ø§Ù„Ø®Ø³Ø§Ø±Ø© ÙˆØ§Ù„Ù…Ø­Ø³Ù†
    criterion = nn.CrossEntropyLoss()
    optimizer = optim.Adam(model.fc.parameters(), lr=0.001)
    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)
    
    # Ù…ØªØºÙŠØ±Ø§Øª Ù„ØªØªØ¨Ø¹ Ø§Ù„Ø£Ø¯Ø§Ø¡
    train_losses = []
    train_accuracies = []
    val_losses = []
    val_accuracies = []
    
    best_val_acc = 0.0
    
    for epoch in range(num_epochs):
        print(f'\nEpoch {epoch+1}/{num_epochs}')
        print('-' * 30)
        
        # Ù…Ø±Ø­Ù„Ø© Ø§Ù„ØªØ¯Ø±ÙŠØ¨
        model.train()
        running_loss = 0.0
        running_corrects = 0
        
        for inputs, labels in tqdm(train_loader, desc="Ø§Ù„ØªØ¯Ø±ÙŠØ¨"):
            inputs = inputs.to(device)
            labels = labels.to(device)
            
            optimizer.zero_grad()
            
            outputs = model(inputs)
            _, preds = torch.max(outputs, 1)
            loss = criterion(outputs, labels)
            
            loss.backward()
            optimizer.step()
            
            running_loss += loss.item() * inputs.size(0)
            running_corrects += torch.sum(preds == labels.data)
        
        epoch_loss = running_loss / len(train_loader.dataset)
        epoch_acc = running_corrects.double() / len(train_loader.dataset)
        
        train_losses.append(epoch_loss)
        train_accuracies.append(epoch_acc.cpu().numpy())
        
        # Ù…Ø±Ø­Ù„Ø© Ø§Ù„ØªÙ‚ÙŠÙŠÙ…
        model.eval()
        val_running_loss = 0.0
        val_running_corrects = 0
        
        with torch.no_grad():
            for inputs, labels in tqdm(val_loader, desc="Ø§Ù„ØªÙ‚ÙŠÙŠÙ…"):
                inputs = inputs.to(device)
                labels = labels.to(device)
                
                outputs = model(inputs)
                _, preds = torch.max(outputs, 1)
                loss = criterion(outputs, labels)
                
                val_running_loss += loss.item() * inputs.size(0)
                val_running_corrects += torch.sum(preds == labels.data)
        
        val_epoch_loss = val_running_loss / len(val_loader.dataset)
        val_epoch_acc = val_running_corrects.double() / len(val_loader.dataset)
        
        val_losses.append(val_epoch_loss)
        val_accuracies.append(val_epoch_acc.cpu().numpy())
        
        print(f'Train Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')
        print(f'Val Loss: {val_epoch_loss:.4f} Acc: {val_epoch_acc:.4f}')
        
        # Ø­ÙØ¸ Ø£ÙØ¶Ù„ Ù†Ù…ÙˆØ°Ø¬
        if val_epoch_acc > best_val_acc:
            best_val_acc = val_epoch_acc
            torch.save(model.state_dict(), 'best_skin_classifier.pt')
            print(f'âœ… ØªÙ… Ø­ÙØ¸ Ø£ÙØ¶Ù„ Ù†Ù…ÙˆØ°Ø¬ Ø¨Ø¯Ù‚Ø©: {best_val_acc:.4f}')
        
        scheduler.step()
    
    # Ø±Ø³Ù… Ù…Ù†Ø­Ù†ÙŠØ§Øª Ø§Ù„ØªØ¯Ø±ÙŠØ¨
    plot_training_curves(train_losses, train_accuracies, val_losses, val_accuracies)
    
    return model

def plot_training_curves(train_losses, train_accuracies, val_losses, val_accuracies):
    """
    Ø±Ø³Ù… Ù…Ù†Ø­Ù†ÙŠØ§Øª Ø§Ù„ØªØ¯Ø±ÙŠØ¨
    Plot training curves
    """
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))
    
    # Ø±Ø³Ù… Ù…Ù†Ø­Ù†Ù‰ Ø§Ù„Ø®Ø³Ø§Ø±Ø©
    ax1.plot(train_losses, label='Train Loss')
    ax1.plot(val_losses, label='Validation Loss')
    ax1.set_title('Ù…Ù†Ø­Ù†Ù‰ Ø§Ù„Ø®Ø³Ø§Ø±Ø© Ø£Ø«Ù†Ø§Ø¡ Ø§Ù„ØªØ¯Ø±ÙŠØ¨')
    ax1.set_xlabel('Epoch')
    ax1.set_ylabel('Loss')
    ax1.legend()
    ax1.grid(True)
    
    # Ø±Ø³Ù… Ù…Ù†Ø­Ù†Ù‰ Ø§Ù„Ø¯Ù‚Ø©
    ax2.plot(train_accuracies, label='Train Accuracy')
    ax2.plot(val_accuracies, label='Validation Accuracy')
    ax2.set_title('Ù…Ù†Ø­Ù†Ù‰ Ø§Ù„Ø¯Ù‚Ø© Ø£Ø«Ù†Ø§Ø¡ Ø§Ù„ØªØ¯Ø±ÙŠØ¨')
    ax2.set_xlabel('Epoch')
    ax2.set_ylabel('Accuracy')
    ax2.legend()
    ax2.grid(True)
    
    plt.tight_layout()
    plt.savefig('training_curves.png')
    plt.show()

def evaluate_model(model, test_loader):
    """
    ØªÙ‚ÙŠÙŠÙ… Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¹Ù„Ù‰ Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø§Ø®ØªØ¨Ø§Ø±
    Evaluate model on test data
    """
    print("\n" + "=" * 50)
    print("Ø§Ù„Ø®Ø·ÙˆØ© 5: ØªÙ‚ÙŠÙŠÙ… Ø§Ù„Ù†Ù…ÙˆØ°Ø¬")
    print("Step 5: Model Evaluation")
    print("=" * 50)
    
    model.eval()
    all_preds = []
    all_labels = []
    
    with torch.no_grad():
        for inputs, labels in tqdm(test_loader, desc="Ø§Ù„ØªÙ‚ÙŠÙŠÙ…"):
            inputs = inputs.to(device)
            labels = labels.to(device)
            
            outputs = model(inputs)
            _, preds = torch.max(outputs, 1)
            
            all_preds.extend(preds.cpu().numpy())
            all_labels.extend(labels.cpu().numpy())
    
    # Ø­Ø³Ø§Ø¨ Ø§Ù„Ø¯Ù‚Ø© Ø§Ù„Ø¥Ø¬Ù…Ø§Ù„ÙŠØ©
    accuracy = np.mean(np.array(all_preds) == np.array(all_labels))
    print(f'Ø¯Ù‚Ø© Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¹Ù„Ù‰ Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø§Ø®ØªØ¨Ø§Ø±: {accuracy:.4f}')
    
    # ØªÙ‚Ø±ÙŠØ± Ø§Ù„ØªØµÙ†ÙŠÙ
    class_names = ['nv', 'mel', 'bkl', 'bcc', 'akiec', 'vasc', 'df']
    print("\nØªÙ‚Ø±ÙŠØ± Ø§Ù„ØªØµÙ†ÙŠÙ Ø§Ù„ØªÙØµÙŠÙ„ÙŠ:")
    print(classification_report(all_labels, all_preds, target_names=class_names))
    
    # Ù…ØµÙÙˆÙØ© Ø§Ù„Ø§Ù„ØªØ¨Ø§Ø³
    cm = confusion_matrix(all_labels, all_preds)
    plt.figure(figsize=(10, 8))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', 
                xticklabels=class_names, yticklabels=class_names)
    plt.title('Ù…ØµÙÙˆÙØ© Ø§Ù„Ø§Ù„ØªØ¨Ø§Ø³')
    plt.xlabel('Ø§Ù„ØªÙ†Ø¨Ø¤')
    plt.ylabel('Ø§Ù„Ø­Ù‚ÙŠÙ‚Ø©')
    plt.tight_layout()
    plt.savefig('confusion_matrix.png')
    plt.show()

def save_model_for_mobile(model):
    """
    Ø­ÙØ¸ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ù„Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù… ÙÙŠ Ø§Ù„ØªØ·Ø¨ÙŠÙ‚ Ø§Ù„Ù…Ø­Ù…ÙˆÙ„
    Save model for mobile application
    """
    print("\n" + "=" * 50)
    print("Ø§Ù„Ø®Ø·ÙˆØ© 6: Ø­ÙØ¸ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ù„Ù„ØªØ·Ø¨ÙŠÙ‚")
    print("Step 6: Saving Model for Mobile App")
    print("=" * 50)
    
    # ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¥Ù„Ù‰ ÙˆØ¶Ø¹ Ø§Ù„ØªÙ‚ÙŠÙŠÙ…
    model.eval()
    
    # Ø¥Ù†Ø´Ø§Ø¡ Ù…Ø«Ø§Ù„ Ù„Ù„Ø¯Ø®Ù„
    example_input = torch.randn(1, 3, 224, 224).to(device)
    
    # ØªØªØ¨Ø¹ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬
    traced_model = torch.jit.trace(model, example_input)
    
    # Ø­ÙØ¸ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…ØªØªØ¨Ø¹
    traced_model.save('skin_classifier_mobile.pt')
    
    print("âœ… ØªÙ… Ø­ÙØ¸ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¨Ù†Ø¬Ø§Ø­ Ù„Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù… ÙÙŠ Ø§Ù„ØªØ·Ø¨ÙŠÙ‚ Ø§Ù„Ù…Ø­Ù…ÙˆÙ„")
    print("Ø§Ø³Ù… Ø§Ù„Ù…Ù„Ù: skin_classifier_mobile.pt")

def test_single_image(model, image_path, transform):
    """
    Ø§Ø®ØªØ¨Ø§Ø± Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¹Ù„Ù‰ ØµÙˆØ±Ø© ÙˆØ§Ø­Ø¯Ø©
    Test model on a single image
    """
    print("\n" + "=" * 50)
    print("Ø§Ø®ØªØ¨Ø§Ø± Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¹Ù„Ù‰ ØµÙˆØ±Ø© ÙˆØ§Ø­Ø¯Ø©")
    print("Testing Model on Single Image")
    print("=" * 50)
    
    # Ø£Ø³Ù…Ø§Ø¡ Ø§Ù„ÙØ¦Ø§Øª
    class_names = {
        0: 'Ø§Ù„Ø´Ø§Ù…Ø§Øª Ø§Ù„ØµØ¨ØºÙŠØ© (Melanocytic nevi)',
        1: 'Ø§Ù„ÙˆØ±Ù… Ø§Ù„Ù…ÙŠÙ„Ø§Ù†ÙŠÙ†ÙŠ Ø§Ù„Ø®Ø¨ÙŠØ« (Melanoma)',
        2: 'Ø§Ù„Ø¢ÙØ§Øª Ø§Ù„Ø­Ù…ÙŠØ¯Ø© Ø§Ù„Ø´Ø¨ÙŠÙ‡Ø© Ø¨Ø§Ù„ØªÙ‚Ø±Ù† (Benign keratosis-like lesions)',
        3: 'Ø³Ø±Ø·Ø§Ù† Ø§Ù„Ø®Ù„Ø§ÙŠØ§ Ø§Ù„Ù‚Ø§Ø¹Ø¯ÙŠØ© (Basal cell carcinoma)',
        4: 'Ø§Ù„ØªÙ‚Ø±Ù† Ø§Ù„Ø´Ø¹Ø§Ø¹ÙŠ (Actinic keratoses)',
        5: 'Ø§Ù„Ø¢ÙØ§Øª Ø§Ù„ÙˆØ¹Ø§Ø¦ÙŠØ© (Vascular lesions)',
        6: 'Ø§Ù„ÙˆØ±Ù… Ø§Ù„Ù„ÙŠÙÙŠ Ø§Ù„Ø¬Ù„Ø¯ÙŠ (Dermatofibroma)'
    }
    
    # ØªØ­Ù…ÙŠÙ„ ÙˆØªØ­Ø¶ÙŠØ± Ø§Ù„ØµÙˆØ±Ø©
    image = Image.open(image_path).convert('RGB')
    image_tensor = transform(image).unsqueeze(0).to(device)
    
    # Ø§Ù„ØªÙ†Ø¨Ø¤
    model.eval()
    with torch.no_grad():
        outputs = model(image_tensor)
        probabilities = torch.nn.functional.softmax(outputs[0], dim=0)
        predicted_class = torch.argmax(outputs, 1).item()
        confidence = probabilities[predicted_class].item()
    
    print(f"Ø§Ù„ØªØ´Ø®ÙŠØµ Ø§Ù„Ù…ØªÙˆÙ‚Ø¹: {class_names[predicted_class]}")
    print(f"Ù†Ø³Ø¨Ø© Ø§Ù„Ø«Ù‚Ø©: {confidence:.2%}")
    
    # Ø¹Ø±Ø¶ Ø£Ø¹Ù„Ù‰ 3 Ø§Ø­ØªÙ…Ø§Ù„Ø§Øª
    print("\nØ£Ø¹Ù„Ù‰ 3 Ø§Ø­ØªÙ…Ø§Ù„Ø§Øª:")
    top3_prob, top3_classes = torch.topk(probabilities, 3)
    for i in range(3):
        class_idx = top3_classes[i].item()
        prob = top3_prob[i].item()
        print(f"{i+1}. {class_names[class_idx]}: {prob:.2%}")

def main():
    """
    Ø§Ù„Ø¯Ø§Ù„Ø© Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ© Ù„ØªØ´ØºÙŠÙ„ Ø§Ù„Ù…Ø´Ø±ÙˆØ¹
    Main function to run the project
    """
    print("ğŸ¥ Ù…Ø´Ø±ÙˆØ¹ ØªØµÙ†ÙŠÙ Ø§Ù„Ø£Ù…Ø±Ø§Ø¶ Ø§Ù„Ø¬Ù„Ø¯ÙŠØ© Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ")
    print("ğŸ¥ Skin Disease Classification using AI")
    print("=" * 60)
    
    # Ø§Ù„Ø®Ø·ÙˆØ© 1: ØªØ­Ù…ÙŠÙ„ ÙˆØ§Ø³ØªÙƒØ´Ø§Ù Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
    df = load_and_explore_data()
    
    # Ø§Ù„Ø®Ø·ÙˆØ© 2: Ø¥Ø¹Ø¯Ø§Ø¯ ØªØ­ÙˆÙŠÙ„Ø§Øª Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
    train_transforms, test_transforms = create_data_transforms()
    
    # ØªÙ‚Ø³ÙŠÙ… Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
    train_df, temp_df = train_test_split(df, test_size=0.3, random_state=42, stratify=df['dx'])
    val_df, test_df = train_test_split(temp_df, test_size=0.5, random_state=42, stratify=temp_df['dx'])
    
    print(f"\nØªÙ‚Ø³ÙŠÙ… Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª:")
    print(f"- Ø§Ù„ØªØ¯Ø±ÙŠØ¨: {len(train_df)} ØµÙˆØ±Ø©")
    print(f"- Ø§Ù„ØªØ­Ù‚Ù‚: {len(val_df)} ØµÙˆØ±Ø©")
    print(f"- Ø§Ù„Ø§Ø®ØªØ¨Ø§Ø±: {len(test_df)} ØµÙˆØ±Ø©")
    
    # Ù…Ù„Ø§Ø­Ø¸Ø©: ÙÙŠ Ø§Ù„ØªØ·Ø¨ÙŠÙ‚ Ø§Ù„Ø­Ù‚ÙŠÙ‚ÙŠØŒ Ø³ØªØ­ØªØ§Ø¬ Ù„Ø¥Ù†Ø´Ø§Ø¡ DataLoaders
    # Ù‡Ù†Ø§ Ù†Ù‚ÙˆÙ… Ø¨Ø¥Ù†Ø´Ø§Ø¡ DataLoaders ÙˆÙ‡Ù…ÙŠØ© Ù„Ù„ØªÙˆØ¶ÙŠØ­
    
    # Ø§Ù„Ø®Ø·ÙˆØ© 3: Ø¨Ù†Ø§Ø¡ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬
    model = create_model()
    
    # Ø§Ù„Ø®Ø·ÙˆØ© 4: ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬
    # ÙÙŠ Ù‡Ø°Ø§ Ø§Ù„Ù…Ø«Ø§Ù„ØŒ Ø³Ù†ØªØ®Ø·Ù‰ Ø§Ù„ØªØ¯Ø±ÙŠØ¨ Ø§Ù„ÙØ¹Ù„ÙŠ ÙˆÙ†Ø­ÙØ¸ Ù†Ù…ÙˆØ°Ø¬ ÙˆÙ‡Ù…ÙŠ
    print("\nâš ï¸  Ù…Ù„Ø§Ø­Ø¸Ø©: ÙÙŠ Ù‡Ø°Ø§ Ø§Ù„Ù…Ø«Ø§Ù„ Ø§Ù„ØªÙˆØ¶ÙŠØ­ÙŠØŒ Ø³Ù†ØªØ®Ø·Ù‰ Ø§Ù„ØªØ¯Ø±ÙŠØ¨ Ø§Ù„ÙØ¹Ù„ÙŠ")
    print("ÙÙŠ Ø§Ù„ØªØ·Ø¨ÙŠÙ‚ Ø§Ù„Ø­Ù‚ÙŠÙ‚ÙŠØŒ Ø³ØªØ­ØªØ§Ø¬ Ù„ØªØ­Ù…ÙŠÙ„ Ø¨ÙŠØ§Ù†Ø§Øª HAM10000 ÙˆØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬")
    
    # Ø§Ù„Ø®Ø·ÙˆØ© 5: Ø­ÙØ¸ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬
    save_model_for_mobile(model)
    
    print("\nğŸ‰ ØªÙ… Ø¥ÙƒÙ…Ø§Ù„ Ø¬Ù…ÙŠØ¹ Ø§Ù„Ø®Ø·ÙˆØ§Øª Ø¨Ù†Ø¬Ø§Ø­!")
    print("ÙŠÙ…ÙƒÙ†Ùƒ Ø§Ù„Ø¢Ù† Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ ÙÙŠ ØªØ·Ø¨ÙŠÙ‚ Flutter")

if __name__ == "__main__":
    main()